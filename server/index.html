<!DOCTYPE html>
<html>
<head>
  <meta http-equiv="Content-Type" content="text/html" charset="utf-8" />
  <title>Blink led With Esp8266 - FingerPose Example</title>

  <!-- Require the peer dependencies of handpose. -->
  <script src="https://unpkg.com/@tensorflow/tfjs-core@3.7.0/dist/tf-core.js"></script>

  <!-- You must explicitly require a TF.js backend if you're not using the tfs union bundle. -->
  <script src="https://unpkg.com/@tensorflow/tfjs-backend-webgl@3.7.0/dist/tf-backend-webgl.js"></script>

  <!-- The main handpose library -->
  <script src="https://cdn.jsdelivr.net/npm/@tensorflow-models/hand-pose-detection@2.0.0/dist/hand-pose-detection.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915/hands.min.js"></script>

  <!-- The fingerpose library -->
  <script src="https://cdn.jsdelivr.net/npm/fingerpose@0.1.0/dist/fingerpose.min.js" type="text/javascript"></script>

  <style>
  * {
      box-sizing: border-box;
      user-select: none;
      font-family: sans-serif;
    }

    body {
      margin: 0;
      padding: 0;
      height: 100vh;
      text-align: center;
      background-color: #111;
    }

    .inform {
      position: absolute;
      left: 50%;
      top: 50%;
      transform: translateX(-50%);
      display: flex;
      justify-content: center;
      color: #fefefe;
    }

    #video-container {
      width: 100vw;
      height: 100vh;
      position: relative;
    }

    .layer {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
    }

    #pose-video {
      transform: scaleX(-1);
    }

    .pose-result {
      font-size: 100px;
      text-align: right;
      padding: 20px 30px 0 0;
    }

    #pose-result-left {
      text-align: left;
    }

    .states {
        position: absolute;
        right: 14%;
        top: 1%;
        width: 300px;
        padding: 12px 24px;
        display: flex;
        align-items: flex-start;
        flex-direction: column;
        gap: 12px;
        text-align: left;
        background: #3333;
        backdrop-filter: blur(50%);
        border: 1px solid rgba( 229, 229, 229, 0.5 );
        box-shadow: 0 0 12px rgba( 229, 229, 229, 0.5 );
        color: #fefefe;

        border-bottom-left-radius: 24px;
        font-size: 32px;
    }

    .states > div {
        width: 100%;
        display: flex;
        align-items: center;
        justify-content: space-between;
    }

    .state {
        width: 64px;
        height: 64px;
        background-color: #ddd;
        border-radius: 50%;
    }

    .state-off {
        background-color: #DC2626;
        border: 1px solid #F87171;
        box-shadow: 0 0 16px #B91C1C;
    }

    .state-on {
        background-color: #22C55E;
        border: 1px solid #15803D;
        box-shadow: 0 0 16px #22C55E;
    }

  </style>
</head>

<body>

  <div class="inform">
    <h1>Carregando c√¢mera, dados, IA...</h1>
  </div>

  <div class="container">
    <div class="video">
      <div id="video-container">
        <video id="pose-video" class="layer" playsinline></video>
        <canvas id="pose-canvas" class="layer"></canvas>
        <div id="pose-result-left" class="layer pose-result"></div>
        <div id="pose-result-right" class="layer pose-result"></div>

        <div class="states">
          <div>
            <span>C√¢mera:</span>
            <span class="state state-off" id="cameraState"></span>
          </div>
          <div>
            <span>L√¢mpada:</span>
            <span class="state state-off" id="lampState"></span>
          </div>
        </div>
      </div>
    </div>
  </div>

<script>
    
    const ws = new WebSocket("ws://localhost:3000");
    const body = document.getElementsByTagName('body');
    const cameraState = document.getElementById('cameraState');
    const lampState = document.getElementById('lampState');
    
    const { GestureDescription, Finger, FingerCurl, FingerDirection } = window.fp;

    const rockGesture = new GestureDescription('rock'); // ‚úäÔ∏è
    const paperGesture = new GestureDescription('paper'); // üñê

    rockGesture.addCurl(Finger.Thumb, FingerCurl.HalfCurl, 1.0);
    rockGesture.addCurl(Finger.Thumb, FingerCurl.NoCurl, 0.5);

    // all other fingers: curled
    for(let finger of [Finger.Index, Finger.Middle, Finger.Ring, Finger.Pinky]) {
        rockGesture.addCurl(finger, FingerCurl.FullCurl, 1.0);
        rockGesture.addCurl(finger, FingerCurl.HalfCurl, 0.9);
    }

    // Paper
    // -----------------------------------------------------------------------------

    // no finger should be curled
    for(let finger of Finger.all) {
        paperGesture.addCurl(finger, FingerCurl.NoCurl, 1.0);
    }

    const gestures = [
      rockGesture, paperGesture
    ];

    const config = {
      video: { width: 640, height: 480, fps: 30 }
    };

    const gestureStrings = {
      'rock': '‚úäÔ∏è',
      'paper': 'üñê',
    };

    async function createDetector() {
      return window.handPoseDetection.createDetector(
        window.handPoseDetection.SupportedModels.MediaPipeHands,
        {
          runtime: "mediapipe",
          modelType: "full",
          maxHands: 2,
          solutionPath: `https://cdn.jsdelivr.net/npm/@mediapipe/hands@0.4.1646424915`,
        }
      )
    }

    async function main() {

      const video = document.querySelector("#pose-video")
      const canvas = document.querySelector("#pose-canvas")
      const ctx = canvas.getContext("2d")
      const resultLayer = {
        right: document.querySelector("#pose-result-right"),
        left: document.querySelector("#pose-result-left")
      }

      // configure gesture estimator
      // add "‚úåüèª" and "üëç" as sample gestures
      const knownGestures = [
        ...gestures
      ]
      const GE = new fp.GestureEstimator(knownGestures)
      // load handpose model
      const detector = await createDetector()
      
      // main estimation loop
      const estimateHands = async () => {

        // clear canvas overlay
        ctx.clearRect(0, 0, config.video.width, config.video.height);
        resultLayer.right.innerText = '';
        resultLayer.left.innerText = '';
        
        // get hand landmarks from video
        const hands = await detector.estimateHands(video, {
          flipHorizontal: true
        });

        for (const hand of hands) {
            const keypoints3D = hand.keypoints3D.map(keypoint => [keypoint.x, keypoint.y, keypoint.z])
            const predictions = GE.estimate(keypoints3D, 9);

            if (predictions.gestures.length > 0) {

              let estadoLed = {
                'rock': "0",
                'paper': "1"
              };

              const result = predictions.gestures.reduce((p, c) => (p.score > c.score) ? p : c);
              ws.send(estadoLed[result.name]);

              if (result.name == 'rock') {
                lampState.classList.remove('state-on');
                lampState.classList.add('state-off');
              }
              else {
                lampState.classList.add('state-on');
                lampState.classList.remove('state-off');
              }

              const found = gestureStrings[result.name]
              // find gesture with highest match score
              const chosenHand = hand.handedness.toLowerCase();

              if(found !== gestureStrings.dont) {
                  resultLayer[chosenHand].innerText = found;
                  continue;
              }
            }
          }

        setTimeout(() => { estimateHands() }, 1000 / config.video.fps);

        if (hands.length == 0) {
          lampState.classList.remove('state-on');
          lampState.classList.add('state-off');

          ws.send("0");
        }
      }

      estimateHands();
    }

    async function initCamera(width, height, fps) {

      const constraints = {
        audio: false,
        video: {
          facingMode: "user",
          width: width,
          height: height,
          frameRate: { max: fps }
        }
      }

      const video = document.querySelector("#pose-video")
      video.width = width
      video.height = height

      // get video stream
      const stream = await navigator.mediaDevices.getUserMedia(constraints)
      video.srcObject = stream

      return new Promise(resolve => {
        video.onloadedmetadata = () => { 
          resolve(video)
        }
      })
    }

    function drawPoint(ctx, x, y, r, color) {
      ctx.beginPath()
      ctx.arc(x, y, r, 0, 2 * Math.PI)
      ctx.fillStyle = color
      ctx.fill()
    }

    window.addEventListener("DOMContentLoaded", () => {

      initCamera(
        config.video.width, config.video.height, config.video.fps
      ).then(video => {
        video.play()
        video.addEventListener("loadeddata", event => {
          cameraState.classList.remove('state-off');
          cameraState.classList.add('state-on');
          main();
        })
      })

      const canvas = document.querySelector("#pose-canvas");
      canvas.width = config.video.width;
      canvas.height = config.video.height;
    })
</script>
</body>
</html>